{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8lFOTW3thzb"
   },
   "source": [
    "# Character-level nanoGPT\n",
    "\n",
    "This is an experiment in training a tiny transformer on character-level data. It's based on a port of nanoGPT — [see the model docs for details](src/experiment/model/README.md).\n",
    "\n",
    "Initially, this experiment was run in a Kaggle notebook. When porting it to run in Modal instead, it started to become hard to work with. The port didn't make it significantly more complicated, but a few things needed to be refactored to run well remotely. So now most of the code lives in modules under [`src/experiment`](src/experiment), and this notebook just ties it together in an way that makes it easy to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from utils.logging import concise_logging\n",
    "\n",
    "\n",
    "def configure_logging():\n",
    "    concise_logging()\n",
    "    logging.getLogger('experiment').setLevel(logging.INFO)\n",
    "    logging.getLogger('subline').setLevel(logging.INFO)\n",
    "    logging.getLogger('utils').setLevel(logging.INFO)\n",
    "    logging.getLogger('mini').setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Configure local logging. This also needs to be done in remote functions.\n",
    "configure_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:58:04.229636Z",
     "iopub.status.busy": "2025-01-08T10:58:04.228795Z",
     "iopub.status.idle": "2025-01-08T10:58:04.605299Z",
     "shell.execute_reply": "2025-01-08T10:58:04.604413Z",
     "shell.execute_reply.started": "2025-01-08T10:58:04.229577Z"
    },
    "id": "YDJ-2iyGt7ve",
    "outputId": "0fd261f9-e2bb-400d-f219-2df7e1ad19a5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from experiment.config import (\n",
    "    AMPConfig,\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    OptimizerConfig,\n",
    "    SchedulerConfig,\n",
    "    TokenizerConfig,\n",
    "    TrainingConfig,\n",
    ")\n",
    "\n",
    "config = TrainingConfig(\n",
    "    model=ModelConfig(\n",
    "        vocab_size=64,  # dummy value, will be set after loading the dataset\n",
    "        block_size=512,\n",
    "        n_embd=32,\n",
    "        n_head=8,\n",
    "        n_head_dim=8,\n",
    "        n_ff=128,\n",
    "        n_layer=15,\n",
    "        dropout=0.2,\n",
    "    ),\n",
    "    tokenizer=TokenizerConfig(\n",
    "        vocabulary=[],  # dummy value, will be set after loading the dataset\n",
    "    ),\n",
    "    data=DataConfig(\n",
    "        batch_size=16,\n",
    "        oversample=1,\n",
    "        train_split=0.8,\n",
    "    ),\n",
    "    optimizer=OptimizerConfig(\n",
    "        weight_decay=1e-1,\n",
    "        learning_rate=0,\n",
    "        betas=(0.9, 0.95),\n",
    "    ),\n",
    "    scheduler=SchedulerConfig(\n",
    "        epochs=100,\n",
    "        warmup_epochs=1,\n",
    "        min_lr_factor=0.1,\n",
    "        # decay_strategy='cosine',\n",
    "    ),\n",
    "    amp=AMPConfig(\n",
    "        enabled=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Default parameters for the @app.function decorator\n",
    "resource_limits: dict[str, Any] = dict(buffer_containers=0, max_containers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for remote execution. Technically we don't need to pre-build the image, but doing so makes the output of later cells cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from experiment.compute.app import data_dir\n",
    "from utils.requirements import freeze\n",
    "from mini.experiment import Experiment\n",
    "\n",
    "exp = Experiment('nanogpt-rope')\n",
    "exp.image = (\n",
    "    modal.Image.debian_slim()\n",
    "    .pip_install(*freeze(all=True, local=False))\n",
    "    .add_local_python_source('experiment', 'utils', 'mini')\n",
    ")  # fmt: skip\n",
    "exp.volumes[data_dir.as_posix()] = volume = modal.Volume.from_name(\n",
    "    'nanogpt-rope-data', create_if_missing=True\n",
    ")  # fmt: skip\n",
    "\n",
    "\n",
    "@exp.run_thither(**resource_limits)\n",
    "async def prebuild():\n",
    "    \"\"\"Forces the image to be built.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "async with exp():\n",
    "    await prebuild()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLKUdJJJ1Kij"
   },
   "source": [
    "# Data\n",
    "\n",
    "We'll grab a small dataset. It's just one big block of text from which we take random substrings. These may overlap, but we aim to take roughly the entire corpus on each epoch.\n",
    "\n",
    "Of note: the \"labels\" $y$ are the same as the input values $x$, shifted by one, since we want to predict each next token.\n",
    "\n",
    "```python\n",
    "x = self.data[idx:idx + self.block_size]\n",
    "y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:58:02.551102Z",
     "iopub.status.busy": "2025-01-08T10:58:02.550811Z",
     "iopub.status.idle": "2025-01-08T10:58:04.181973Z",
     "shell.execute_reply": "2025-01-08T10:58:04.181085Z",
     "shell.execute_reply.started": "2025-01-08T10:58:02.551075Z"
    },
    "id": "IOLkzyifA-4o",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "a0827ffc-b521-43cb-9f7f-2029478f2735",
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from experiment.config import DatasetMetadata\n",
    "from utils.param_types import validate_call\n",
    "\n",
    "\n",
    "@validate_call\n",
    "def download_the_scarlet_pimpernel() -> tuple[str, DatasetMetadata]:\n",
    "    import ftfy\n",
    "    import requests\n",
    "\n",
    "    url = 'https://www.gutenberg.org/cache/epub/60/pg60.txt'\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    text = response.text.replace('\\r\\n', '\\n')\n",
    "    text = text[text.find('\\nCHAPTER I.') : text.rfind('*** END OF THE PROJECT GUTENBERG EBOOK')].strip()\n",
    "    # Normalize text to avoid weird quotation marks etc.\n",
    "    text, explanation = ftfy.fix_and_explain(text)\n",
    "    metadata = DatasetMetadata(\n",
    "        title='The Scarlet Pimpernel',\n",
    "        url=url,\n",
    "        fixes=explanation or [],\n",
    "        total_chars=len(text),\n",
    "    )\n",
    "    return text, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment.utils import align\n",
    "\n",
    "\n",
    "@exp.run_thither(**resource_limits)\n",
    "async def prepare_data():\n",
    "    from experiment.compute.data_pipelines import save_data\n",
    "    from experiment.data.preparation import tokenize_data\n",
    "\n",
    "    configure_logging()\n",
    "\n",
    "    sources = [\n",
    "        download_the_scarlet_pimpernel(),\n",
    "    ]\n",
    "    data, metadata = tokenize_data(sources)\n",
    "    save_data(data, metadata)\n",
    "    volume.commit()\n",
    "    return metadata\n",
    "\n",
    "\n",
    "async with exp():\n",
    "    input_metadata = await prepare_data()\n",
    "\n",
    "config.tokenizer = input_metadata.tokenizer_config.model_copy()\n",
    "config.model.vocab_size = align(config.tokenizer.vocab_size, 64)\n",
    "input_metadata.model_dump(exclude={'tokenizer_config'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX0gi9KR1ll4"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:58:04.607245Z",
     "iopub.status.busy": "2025-01-08T10:58:04.606863Z",
     "iopub.status.idle": "2025-01-08T10:58:18.274334Z",
     "shell.execute_reply": "2025-01-08T10:58:18.273593Z",
     "shell.execute_reply.started": "2025-01-08T10:58:04.607206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from contextlib import asynccontextmanager\n",
    "from mini.hither import Callback, run_hither\n",
    "from mini.utils import coerce_to_async\n",
    "from utils.lr_finder.types import LRFinderConfig, LRFinderSeries, Progress\n",
    "from utils.param_types import validate_call\n",
    "\n",
    "\n",
    "@exp.run_thither(gpu='L4', **resource_limits)\n",
    "async def find_learning_rate(plot: Callback[LRFinderConfig | LRFinderSeries], prog: Callback[Progress]):\n",
    "    import torch\n",
    "    from experiment.compute.data_pipelines import load_data\n",
    "    from experiment.data.dataloader import get_dataloader\n",
    "    from experiment.model.gpt import GPT\n",
    "    from experiment.training.optimizer import configure_optimizer\n",
    "    from utils.lr_finder.lr_finder import lr_finder_search\n",
    "    from utils.torch.mixed_precision import AMPContext\n",
    "    from utils.torch.types import get_device\n",
    "\n",
    "    configure_logging()\n",
    "\n",
    "    model: torch.nn.Module = GPT(config.model)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = configure_optimizer(model, config.optimizer)\n",
    "    data, _ = load_data()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        data = data.cuda()\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    train_loader, _ = get_dataloader(data, model_config=config.model, data_config=config.data)\n",
    "    amp_context = AMPContext(use_amp=config.amp.enabled, device_type=get_device(model), dtype=config.amp.dtype)\n",
    "\n",
    "    for event in lr_finder_search(model, criterion, optimizer, train_loader, amp_context=amp_context):\n",
    "        match event:\n",
    "            case LRFinderConfig() | LRFinderSeries() as data:\n",
    "                plot(data)\n",
    "            case Progress() as progress:\n",
    "                prog(progress)\n",
    "            case float() as suggested_lr:\n",
    "                return suggested_lr\n",
    "    raise ValueError('No suggested learning rate found.')\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def progress():\n",
    "    from utils.nb import displayer\n",
    "\n",
    "    total_steps = 1\n",
    "\n",
    "    display = displayer()\n",
    "\n",
    "    async def _progress(event: Progress):\n",
    "        nonlocal total_steps\n",
    "        if event.total_steps:\n",
    "            total_steps = event.total_steps\n",
    "        suffix = f' - {event.info}' if event.info else ''\n",
    "        if event.step:\n",
    "            fraction = event.step / total_steps\n",
    "            display(f'Progress: {fraction:.0%}{suffix}')\n",
    "\n",
    "    yield _progress\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def plotter():\n",
    "    from utils.lr_finder.vis import lr_finder_plot\n",
    "\n",
    "    yield coerce_to_async(lr_finder_plot())\n",
    "\n",
    "\n",
    "async with exp(), run_hither(progress) as prog, run_hither(plotter) as plot:\n",
    "    suggested_lr = await find_learning_rate(plot, prog)\n",
    "\n",
    "print(f'Suggested Learning Rate: {suggested_lr:.2e}')\n",
    "config.optimizer.learning_rate = suggested_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:58:18.276358Z",
     "iopub.status.busy": "2025-01-08T10:58:18.275560Z",
     "iopub.status.idle": "2025-01-08T11:02:48.670789Z",
     "shell.execute_reply": "2025-01-08T11:02:48.669885Z",
     "shell.execute_reply.started": "2025-01-08T10:58:18.276317Z"
    },
    "id": "YDJ-2iyGt7ve",
    "outputId": "0fd261f9-e2bb-400d-f219-2df7e1ad19a5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from experiment.compute.model import save_checkpoint\n",
    "from experiment.compute.training import TrainingEvent, train_model\n",
    "from utils.time import duration\n",
    "\n",
    "\n",
    "@exp.run_thither(gpu='L4', timeout=int(duration('20 min')), **resource_limits)\n",
    "async def train(prog_cb: Callback[TrainingEvent]):\n",
    "    configure_logging()\n",
    "\n",
    "    for event in train_model(config):\n",
    "        match event:\n",
    "            case 'checkpoint', (model, cfg, metrics):\n",
    "                save_checkpoint(model, cfg, metrics)\n",
    "                volume.commit()\n",
    "            case _:\n",
    "                prog_cb(event)\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def progress():\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    with tqdm(total=0, desc='Epoch') as pb_epoch, tqdm(total=0, desc='Step', leave=False) as pb_step:\n",
    "\n",
    "        async def progress_callback(event: TrainingEvent):\n",
    "            match event:\n",
    "                case 'epochs', total:\n",
    "                    pb_epoch.total = total\n",
    "                    pb_epoch.refresh()\n",
    "                case 'steps-per-epoch', total:\n",
    "                    pb_step.total = total\n",
    "                    pb_step.refresh()\n",
    "                case ('train-step', n) | ('val-step', n):\n",
    "                    pb_step.update(n)\n",
    "                case 'epoch-end', metrics:\n",
    "                    pb_epoch.set_postfix(metrics.model_dump())\n",
    "                    pb_epoch.update(1)\n",
    "                    pb_step.reset()\n",
    "                    # plot(metrics.val_loss)\n",
    "\n",
    "        yield progress_callback\n",
    "\n",
    "\n",
    "async with run_hither(progress) as prog_cb, exp():\n",
    "    await train(prog_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ig1r_FS_OwT"
   },
   "source": [
    "# Generate continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import NonNegativeFloat, PositiveInt\n",
    "from experiment.model.gpt import Generation\n",
    "\n",
    "\n",
    "@exp.run_thither(gpu='T4', timeout=int(duration('1 min')), **resource_limits)\n",
    "@validate_call\n",
    "async def generate(\n",
    "    prompts: list[str],\n",
    "    max_new_tokens: PositiveInt,\n",
    "    temperature: NonNegativeFloat,\n",
    ") -> tuple[list[list[str]], Generation]:\n",
    "    from typing import cast\n",
    "    import torch\n",
    "\n",
    "    from experiment.compute.model import load_checkpoint\n",
    "    from experiment.data.tokenizer import CharTokenizer\n",
    "\n",
    "    configure_logging()\n",
    "\n",
    "    print('Loading model from checkpoint')\n",
    "    model, config, _ = load_checkpoint()\n",
    "    model.eval()\n",
    "    tokenizer = CharTokenizer(config.tokenizer)\n",
    "    context = torch.tensor(tokenizer.encode(prompts), dtype=torch.long)\n",
    "    if torch.cuda.is_available():\n",
    "        context = context.cuda()\n",
    "        model = model.cuda()\n",
    "\n",
    "    print(f'Generating {max_new_tokens} tokens with temperature {temperature}')\n",
    "    output = model.generate(context, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    print(f'Generated {len(output.tokens)} tokens')\n",
    "\n",
    "    toks = cast(list[list[int]], output.tokens.tolist())\n",
    "    return tokenizer.decode_each(toks), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:02:48.672114Z",
     "iopub.status.busy": "2025-01-08T11:02:48.671852Z",
     "iopub.status.idle": "2025-01-08T11:02:56.581146Z",
     "shell.execute_reply": "2025-01-08T11:02:56.580225Z",
     "shell.execute_reply.started": "2025-01-08T11:02:48.672088Z"
    },
    "id": "tTHxOrzbGCgT",
    "outputId": "c1804efc-c2dd-4a58-d119-3c56d354f722",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompts = [\"Odd's fish m'dear,\"]\n",
    "\n",
    "async with exp():\n",
    "    continuations, metadata = await generate(prompts=prompts, max_new_tokens=500, temperature=0.5)\n",
    "\n",
    "for sequence in continuations:\n",
    "    print(''.join(sequence[:40]), '(etc.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token metrics: Perplexity and entropy\n",
    "\n",
    "Let's visualize the generation along with some metrics:\n",
    "* **Entropy** is how diffuse the probability distribution is for the next token, i.e. the spread of probabilities before sampling. Can be thought of as how uncertain the model is about what to say next. This usually isn't calculated for prompt tokens.\n",
    "* **Perplexity** is how unlikely the next token is. Can be thought of as how surprised the model is by the presence of the token at this point in the sequence. This is usually calculated for prompt tokens, but can also be calculated for continuation tokens.\n",
    "\n",
    "```python\n",
    "# For entropy (uncertainty about next token):\n",
    "probs = F.softmax(next_token_logits, dim=-1)\n",
    "entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)\n",
    "\n",
    "# For perplexity (surprise about actual token):\n",
    "token_loss = F.cross_entropy(next_token_logits, idx_next.view(-1), reduction='none')\n",
    "perplexities[curr_len] = torch.exp(token_loss)\n",
    "```\n",
    "\n",
    "Notably, _the entropy of continuation tokens is unaffected by temperature_, whereas the perplexity _is_ affected (because it's calculated after sampling).\n",
    "\n",
    "We should expect them to be correlated, because when the model is very certain (low entropy), it's more likely to sample a high-probability token (low perplexity), and vice-versa. But they can diverge in interesting ways: You could have high entropy but still sample a high-probability token by chance (high entropy, low perplexity), or You could have low entropy but sample an unlikely token due to temperature (low entropy, high perplexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:02:56.582260Z",
     "iopub.status.busy": "2025-01-08T11:02:56.582025Z",
     "iopub.status.idle": "2025-01-08T11:02:56.666205Z",
     "shell.execute_reply": "2025-01-08T11:02:56.665504Z",
     "shell.execute_reply.started": "2025-01-08T11:02:56.582238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from experiment.model.gpt import SingleGeneration\n",
    "\n",
    "\n",
    "def annotate_tokens(tokens: list[str], metadata: SingleGeneration):\n",
    "    from subline.series import Series\n",
    "    from subline.subline import Subline\n",
    "    from IPython.display import SVG, display\n",
    "\n",
    "    viz = Subline(chars_per_line=80)\n",
    "    svg = viz.plot(\n",
    "        tokens,\n",
    "        [\n",
    "            # EntropySeries(metadata.entropy, label='Entropy', vocab_size=metadata.vocab_size),\n",
    "            # EntropySeries(metadata.surprisal, label='Surprisal', vocab_size=metadata.vocab_size),\n",
    "            Series(metadata.surprise_surprise, label='S₂'),\n",
    "            Series(-metadata.surprise_surprise, label='-S₂', dasharray='1'),\n",
    "        ],\n",
    "    )\n",
    "    display(SVG(svg))\n",
    "\n",
    "\n",
    "annotate_tokens(continuations[0], metadata[0])\n",
    "\n",
    "# TODO: Is the quality worse now, using fp16? It *seems* worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future research\n",
    "\n",
    "### Temperature as a resource\n",
    "\n",
    "It's interesting to note that the first character of each word is high entropy and high perplexity, and subsequent characters are lower. And where the model makes spelling mistakes, it often had low entropy but then high perplexity! Which suggests that it knows what it wants to write, but the sampling mechanism is messing it up.\n",
    "\n",
    "I'd like to see if it improves to lower the temperature after the first letter of each word. How would that generalise to languages that don't use spaces? Perhaps the temperature could be a resource that gets used (by picking unlikely tokens) and gradually replenished (by picking likely ones). Yeah that could be really cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Karpathy, A. (2022). nanoGPT [Computer software]. GitHub. https://github.com/karpathy/nanoGPT\n",
    "\n",
    "Sanderson, G. (2024a). Visualizing attention, a transformer's heart. 3Blue1Brown. https://www.3blue1brown.com/lessons/attention\n",
    "\n",
    "Sanderson, G. (2024b). How might LLMs store facts. 3Blue1Brown. https://www.3blue1brown.com/lessons/mlp\n",
    "\n",
    "# Software Licenses\n",
    "\n",
    "The code in this notebook is derived from nanoGPT (Karpathy, 2022), which is licensed under the MIT License, Copyright (c) 2022 Andrej Karpathy."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4019756,
     "sourceId": 6993633,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 216420817,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
