{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8lFOTW3thzb"
   },
   "source": [
    "# Character-level nanoGPT\n",
    "\n",
    "This is an experiment in training a tiny transformer on character-level data. It's based on a port of nanoGPT â€” [see the model docs for details](../src/experiment/model/README.md).\n",
    "\n",
    "Initially, this experiment was run in a Kaggle notebook. When porting it to run in Modal instead, it started to become hard to work with. The port didn't make it significantly more complicated, but a few things needed to be refactored to run well remotely. So now most of the code lives in modules under [src/experiment](../src/experiment), and this notebook just ties it together in an way that makes it easy to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from utils.logging import SimpleLoggingConfig\n",
    "\n",
    "logging_config = SimpleLoggingConfig().info('notebook', 'experiment', 'mini', 'subline', 'utils')\n",
    "logging_config.apply()\n",
    "\n",
    "log = logging.getLogger('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use structured classes to make it easier to save them alongside model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:58:04.229636Z",
     "iopub.status.busy": "2025-01-08T10:58:04.228795Z",
     "iopub.status.idle": "2025-01-08T10:58:04.605299Z",
     "shell.execute_reply": "2025-01-08T10:58:04.604413Z",
     "shell.execute_reply.started": "2025-01-08T10:58:04.229577Z"
    },
    "id": "YDJ-2iyGt7ve",
    "outputId": "0fd261f9-e2bb-400d-f219-2df7e1ad19a5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from experiment.config import (\n",
    "    MixedPrecisionConfig,\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    OptimizerConfig,\n",
    "    SchedulerConfig,\n",
    "    TokenizerConfig,\n",
    "    TrainingConfig,\n",
    ")\n",
    "\n",
    "config = TrainingConfig(\n",
    "    model=ModelConfig(\n",
    "        vocab_size=64,  # will be set after loading the dataset\n",
    "        block_size=512,\n",
    "        n_embd=32,\n",
    "        n_head=8,\n",
    "        n_head_dim=8,\n",
    "        n_ff=128,\n",
    "        n_layer=12,\n",
    "        dropout=0.1,\n",
    "    ),\n",
    "    tokenizer=TokenizerConfig(\n",
    "        vocabulary=[],  # will be set after loading the dataset\n",
    "    ),\n",
    "    data=DataConfig(\n",
    "        batch_size=16,\n",
    "        oversample=1,\n",
    "        train_split=0.8,\n",
    "        padding_chance=0.1,\n",
    "    ),\n",
    "    optimizer=OptimizerConfig(\n",
    "        weight_decay=1e-3,\n",
    "        learning_rate=0,  # will be set after searching for the best learning rate\n",
    "        betas=(0.9, 0.95),\n",
    "    ),\n",
    "    scheduler=SchedulerConfig(\n",
    "        epochs=100,\n",
    "        warmup_epochs=10,\n",
    "        min_lr_factor=0.01,\n",
    "        # decay_strategy='cosine',\n",
    "    ),\n",
    "    amp=MixedPrecisionConfig(\n",
    "        enabled=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Default parameters for the @app.function decorator\n",
    "resource_limits: dict[str, Any] = dict(buffer_containers=0, max_containers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the remote environment to include the dependencies and local modules. Note that we don't need to include `subline`, because it's only used locally. Other local deps like `matplotlib` are marked as local-only in [pyproject.toml](../pyproject.toml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from experiment.compute.app import data_dir\n",
    "from utils.requirements import freeze, project_packages\n",
    "from mini.experiment import Experiment\n",
    "\n",
    "run = Experiment('nanogpt')\n",
    "\n",
    "run.image = (\n",
    "    modal.Image.debian_slim()\n",
    "    .pip_install(*freeze(all=True, local=False))\n",
    "    .add_local_python_source(*project_packages())\n",
    ")  # fmt: skip\n",
    "\n",
    "run.volumes[data_dir.as_posix()] = volume = modal.Volume.from_name(\n",
    "    'nanogpt-data',\n",
    "    create_if_missing=True,\n",
    ")  # fmt: skip\n",
    "\n",
    "_ = run.before_each(logging_config.apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a stale build can cause issues. If a `@run.thither` call gets stuck, you can force a rebuild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.image.force_build = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLKUdJJJ1Kij"
   },
   "source": [
    "# Data\n",
    "\n",
    "We'll grab a small dataset. It's just one big block of text from which we take random substrings. These may overlap, but we aim to take roughly the entire corpus on each epoch.\n",
    "\n",
    "Of note: the \"labels\" $y$ are the same as the input values $x$, shifted by one, since we want to predict each next token.\n",
    "\n",
    "```python\n",
    "x = self.data[idx:idx + self.block_size]\n",
    "y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:58:02.551102Z",
     "iopub.status.busy": "2025-01-08T10:58:02.550811Z",
     "iopub.status.idle": "2025-01-08T10:58:04.181973Z",
     "shell.execute_reply": "2025-01-08T10:58:04.181085Z",
     "shell.execute_reply.started": "2025-01-08T10:58:02.551075Z"
    },
    "id": "IOLkzyifA-4o",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "a0827ffc-b521-43cb-9f7f-2029478f2735",
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from experiment.config import DatasetMetadata\n",
    "from utils.param_types import validate_call\n",
    "\n",
    "\n",
    "@validate_call\n",
    "def download_the_scarlet_pimpernel() -> tuple[str, DatasetMetadata]:\n",
    "    import ftfy\n",
    "    import requests\n",
    "\n",
    "    url = 'https://www.gutenberg.org/cache/epub/60/pg60.txt'\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    text = response.text.replace('\\r\\n', '\\n')\n",
    "    text = text[text.find('\\nCHAPTER I.') : text.rfind('*** END OF THE PROJECT GUTENBERG EBOOK')].strip()\n",
    "    # Normalize text to avoid weird quotation marks etc.\n",
    "    text, explanation = ftfy.fix_and_explain(text)\n",
    "    metadata = DatasetMetadata(\n",
    "        title='The Scarlet Pimpernel',\n",
    "        url=url,\n",
    "        fixes=explanation or [],\n",
    "        total_chars=len(text),\n",
    "    )\n",
    "    return text, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment.utils import align\n",
    "\n",
    "\n",
    "@run.thither(**resource_limits)\n",
    "async def prepare_data():\n",
    "    from experiment.compute.data_pipelines import save_data\n",
    "    from experiment.data.preparation import tokenize_data\n",
    "\n",
    "    sources = [\n",
    "        download_the_scarlet_pimpernel(),\n",
    "    ]\n",
    "    data, metadata = tokenize_data(sources)\n",
    "    save_data(data, metadata)\n",
    "    volume.commit()\n",
    "    return metadata\n",
    "\n",
    "\n",
    "async with run():\n",
    "    input_metadata = await prepare_data()\n",
    "\n",
    "config.tokenizer = input_metadata.tokenizer_config.model_copy()\n",
    "config.model.vocab_size = align(config.tokenizer.vocab_size, 64)\n",
    "input_metadata.model_dump(exclude={'tokenizer_config'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX0gi9KR1ll4"
   },
   "source": [
    "# Training\n",
    "\n",
    "First, we'll run a few epochs of training to find a good learning rate. This uses a custom LR finder that progressively narrows the search space. Doing it this way improves the stability of discovered learning rates. The `@run.hither` functions `plot` and `progress` give visual feedback directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:58:04.607245Z",
     "iopub.status.busy": "2025-01-08T10:58:04.606863Z",
     "iopub.status.idle": "2025-01-08T10:58:18.274334Z",
     "shell.execute_reply": "2025-01-08T10:58:18.273593Z",
     "shell.execute_reply.started": "2025-01-08T10:58:04.607206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from contextlib import asynccontextmanager\n",
    "from mini.hither import Callback\n",
    "from mini.utils import coerce_to_async\n",
    "from utils.lr_finder.types import LRFinderConfig, LRFinderSeries, Progress\n",
    "from utils.param_types import validate_call\n",
    "\n",
    "\n",
    "@run.thither(gpu='L4', **resource_limits)\n",
    "async def find_learning_rate(plot: Callback[LRFinderConfig | LRFinderSeries], prog: Callback[Progress]):\n",
    "    # This function is run in a remote container\n",
    "    import torch\n",
    "    from experiment.compute.data_pipelines import load_data\n",
    "    from experiment.data.dataloader import get_dataloader\n",
    "    from experiment.model.gpt import GPT\n",
    "    from experiment.training.optimizer import configure_optimizer\n",
    "    from utils.lr_finder.lr_finder import lr_finder_search\n",
    "    from utils.torch.mixed_precision import AMPContext\n",
    "    from utils.torch.types import get_device\n",
    "\n",
    "    model: torch.nn.Module = GPT(config.model)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = configure_optimizer(model, config.optimizer)\n",
    "    data, _ = load_data()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        data = data.cuda()\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    train_loader, _ = get_dataloader(data, model_config=config.model, data_config=config.data)\n",
    "    amp_context = AMPContext(use_amp=config.amp.enabled, device_type=get_device(model), dtype=config.amp.dtype)\n",
    "\n",
    "    for event in lr_finder_search(model, criterion, optimizer, train_loader, amp_context=amp_context):\n",
    "        match event:\n",
    "            case LRFinderConfig() | LRFinderSeries() as data:\n",
    "                plot(data)\n",
    "            case Progress() as progress:\n",
    "                prog(progress)\n",
    "            case float() as suggested_lr:\n",
    "                return suggested_lr\n",
    "    raise ValueError('No suggested learning rate found.')\n",
    "\n",
    "\n",
    "@run.hither\n",
    "@asynccontextmanager\n",
    "async def progress():\n",
    "    # Factory function to create a local progress callback\n",
    "    from utils.nb import displayer\n",
    "\n",
    "    total_steps = 1\n",
    "\n",
    "    display = displayer()\n",
    "\n",
    "    async def _progress(event: Progress):\n",
    "        # A stub of this function is passed to the remote container\n",
    "        nonlocal total_steps\n",
    "        if event.total_steps:\n",
    "            total_steps = event.total_steps\n",
    "        suffix = f' - {event.info}' if event.info else ''\n",
    "        if event.step:\n",
    "            fraction = event.step / total_steps\n",
    "            display(f'Progress: {fraction:.0%}{suffix}')\n",
    "\n",
    "    yield _progress\n",
    "\n",
    "\n",
    "@run.hither\n",
    "@asynccontextmanager\n",
    "async def plotter():\n",
    "    # Factory function to create a local visualization\n",
    "    from utils.lr_finder.vis import lr_finder_plot\n",
    "\n",
    "    # A stub of this function is passed to the remote container\n",
    "    yield coerce_to_async(lr_finder_plot())\n",
    "\n",
    "\n",
    "# Create local function stubs\n",
    "async with run(), progress() as prog, plotter() as plot:\n",
    "    # Run the remote function\n",
    "    suggested_lr = await find_learning_rate(plot, prog)\n",
    "\n",
    "print(f'Suggested Learning Rate: {suggested_lr:.2e}')\n",
    "config.optimizer.learning_rate = suggested_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found a good learning rate, let's jump into a full training run! We'll use a decent GPU, and save the model to a remote volume. Locally, we'll just show training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:58:18.276358Z",
     "iopub.status.busy": "2025-01-08T10:58:18.275560Z",
     "iopub.status.idle": "2025-01-08T11:02:48.670789Z",
     "shell.execute_reply": "2025-01-08T11:02:48.669885Z",
     "shell.execute_reply.started": "2025-01-08T10:58:18.276317Z"
    },
    "id": "YDJ-2iyGt7ve",
    "outputId": "0fd261f9-e2bb-400d-f219-2df7e1ad19a5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from experiment.compute.model import save_checkpoint\n",
    "from experiment.compute.training import TrainingEvent, train_model\n",
    "from utils.time import duration\n",
    "\n",
    "\n",
    "@run.thither(gpu='L4', timeout=int(duration('20 min')), **resource_limits)\n",
    "async def train(prog_cb: Callback[TrainingEvent]):\n",
    "    # This function is run in a remote container\n",
    "    for event in train_model(config):\n",
    "        match event:\n",
    "            case 'checkpoint', (model, cfg, metrics):\n",
    "                save_checkpoint(model, cfg, metrics)\n",
    "                volume.commit()\n",
    "            case _:\n",
    "                prog_cb(event)\n",
    "\n",
    "\n",
    "@run.hither\n",
    "@asynccontextmanager\n",
    "async def progress():\n",
    "    # Factory function to create a local progress callback\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    with tqdm(total=1, desc='Epoch') as pb_epoch, tqdm(total=1, desc='Step', leave=False) as pb_step:\n",
    "\n",
    "        async def progress_callback(event: TrainingEvent):\n",
    "            # A stub of this function is passed to the remote container\n",
    "            match event:\n",
    "                case 'epochs', total:\n",
    "                    pb_epoch.total = total\n",
    "                    pb_epoch.refresh()\n",
    "                case 'steps-per-epoch', total:\n",
    "                    pb_step.total = total\n",
    "                    pb_step.refresh()\n",
    "                case ('train-step', n) | ('val-step', n):\n",
    "                    pb_step.update(n)\n",
    "                case 'epoch-end', metrics:\n",
    "                    pb_epoch.set_postfix(metrics.model_dump())\n",
    "                    pb_epoch.update(1)\n",
    "                    pb_step.reset()\n",
    "                    # plot(metrics.val_loss)\n",
    "\n",
    "        # By yielding the callback instead of returning it, the context manager\n",
    "        # takes care of closing the tqdm progress bars after the run.\n",
    "        yield progress_callback\n",
    "\n",
    "\n",
    "# Create local function stubs\n",
    "async with progress() as prog_cb, run():\n",
    "    # Run the remote function\n",
    "    await train(prog_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ig1r_FS_OwT"
   },
   "source": [
    "# Generate continuations\n",
    "\n",
    "Inference is also run remotely, which has some benefits:\n",
    "- We can use a GPU (although we pick a low-spec one)\n",
    "- We don't have to download the model (which could be very large!)\n",
    "\n",
    "Only the results are sent back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import NonNegativeFloat, PositiveInt\n",
    "\n",
    "\n",
    "@run.thither(gpu='T4', timeout=int(duration('1 min')), **resource_limits)\n",
    "@validate_call\n",
    "async def generate(\n",
    "    prompts: list[str],\n",
    "    max_new_tokens: PositiveInt,\n",
    "    temperature: NonNegativeFloat,\n",
    "):\n",
    "    # This function runs remotely too, because the model is a bit large.\n",
    "    # It's mounted as a volume; see environment setup above.\n",
    "    from typing import cast\n",
    "    import torch\n",
    "\n",
    "    from experiment.compute.model import load_checkpoint\n",
    "    from experiment.data.tokenizer import CharTokenizer\n",
    "\n",
    "    log.info('Loading model from checkpoint')\n",
    "    model, config, _ = load_checkpoint()\n",
    "    model.eval()\n",
    "    tokenizer = CharTokenizer(config.tokenizer)\n",
    "    context = torch.tensor(tokenizer.encode(prompts, config.model.block_size), dtype=torch.long)\n",
    "    if torch.cuda.is_available():\n",
    "        context = context.cuda()\n",
    "        model = model.cuda()\n",
    "\n",
    "    log.info(f'Generating {max_new_tokens} tokens with temperature {temperature}')\n",
    "    output = model.generate(context, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "\n",
    "    toks = cast(list[list[int]], output.tokens.tolist())\n",
    "    return tokenizer.decode_each(toks), output\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    'Sir Percy Bla',\n",
    "    'She looked at Sir Andrew with eager curi',\n",
    "]\n",
    "\n",
    "# Set up context\n",
    "async with run():\n",
    "    # Generate remotely, and bring the results back\n",
    "    continuations, metadata = await generate(prompts=prompts, max_new_tokens=300, temperature=0.5)\n",
    "\n",
    "# Show a sample of each generation\n",
    "for sequence in continuations:\n",
    "    print(''.join(sequence)[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token metrics: Surprisal and entropy\n",
    "\n",
    "Let's visualize the generation along with some metrics:\n",
    "* **Entropy** is how diffuse the probability distribution is for the next token, i.e. the spread of probabilities _before_ sampling. Can be thought of as how uncertain the model is about what to say next. This usually isn't calculated for prompt tokens, but there's nothing stopping us.\n",
    "* **Surprisal** is how unlikely each the next token is. If this was calculated for the whole sequence we would call it \"perplexity\", but we want to look deeper. It's literally the cross-entropy loss, which is used as the training signal. During inference, it can be thought of as how surprised the model is by the presence each the token at its location in the sequence.\n",
    "\n",
    "Together, surprisal and entropy can help us to understand:\n",
    "- For **ground-truth** text: how well the model fits the distribution\n",
    "- For **prompts**: how weird or out-of-distribution they are\n",
    "- For **continuations**: how the prompt and temperature affect generation.\n",
    "\n",
    "```python\n",
    "# For entropy (uncertainty)\n",
    "probs = F.softmax(next_token_logits, ...)\n",
    "entropy = -torch.sum(probs * torch.log(probs), ...)\n",
    "\n",
    "# For surprisal\n",
    "token_loss = F.cross_entropy(next_token_logits, ...)\n",
    "surprisals[i_next] = token_loss\n",
    "```\n",
    "\n",
    "Notably, _the entropy of continuation tokens is unaffected by temperature_, whereas the surprisal _is_ affected (because it's calculated after sampling).\n",
    "\n",
    "We should expect them to be correlated, because when the model is very certain (low entropy), it's more likely to sample a high-probability token (low surprisal), and vice-versa. But they can diverge in interesting ways: You could have high entropy but still sample a high-probability token by chance (high entropy, low surprisal), or you could have low entropy but sample an unlikely token due to temperature (low entropy, high surprisal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:02:56.582260Z",
     "iopub.status.busy": "2025-01-08T11:02:56.582025Z",
     "iopub.status.idle": "2025-01-08T11:02:56.666205Z",
     "shell.execute_reply": "2025-01-08T11:02:56.665504Z",
     "shell.execute_reply.started": "2025-01-08T11:02:56.582238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from experiment.model.gpt import SingleGeneration\n",
    "\n",
    "\n",
    "def annotate_tokens(tokens: list[str], metadata: SingleGeneration):\n",
    "    from subline.series import Series\n",
    "    from subline.subline import Subline\n",
    "    from IPython.display import SVG, display\n",
    "\n",
    "    viz = Subline(chars_per_line=80)\n",
    "    svg = viz.plot(\n",
    "        tokens,\n",
    "        [\n",
    "            # EntropySeries(metadata.entropy, label='Entropy', vocab_size=metadata.vocab_size),\n",
    "            # EntropySeries(metadata.surprisal, label='Surprisal', vocab_size=metadata.vocab_size),\n",
    "            Series(metadata.surprise_surprise, label='Sâ‚‚'),\n",
    "            Series(-metadata.surprise_surprise, label='-Sâ‚‚', dasharray='1'),\n",
    "        ],\n",
    "    )\n",
    "    display(SVG(svg))\n",
    "\n",
    "\n",
    "annotate_tokens(continuations[0], metadata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future research\n",
    "\n",
    "### Temperature as a resource\n",
    "\n",
    "It's interesting to note that the first character of each word is high entropy and high surprisal, and subsequent characters are lower. And where the model makes spelling mistakes, it often had low entropy but then high surprisal â€” which suggests that it knows what it wants to write, but the sampling mechanism is messing it up.\n",
    "\n",
    "I'd like to see if it improves to lower the temperature after the first letter of each word. How would that generalise to languages that don't use spaces? Perhaps the temperature could be a resource that gets consumed (e.g. by picking unlikely tokens) and gradually replenished (by picking likely ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Karpathy, A. (2022). nanoGPT [Computer software]. GitHub. https://github.com/karpathy/nanoGPT\n",
    "\n",
    "Sanderson, G. (2024a). Visualizing attention, a transformer's heart. 3Blue1Brown. https://www.3blue1brown.com/lessons/attention\n",
    "\n",
    "Sanderson, G. (2024b). How might LLMs store facts. 3Blue1Brown. https://www.3blue1brown.com/lessons/mlp\n",
    "\n",
    "# Software Licenses\n",
    "\n",
    "The code in this notebook is derived from nanoGPT (Karpathy, 2022), which is licensed under the MIT License, Copyright (c) 2022 Andrej Karpathy."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4019756,
     "sourceId": 6993633,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 216420817,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mi-ni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
